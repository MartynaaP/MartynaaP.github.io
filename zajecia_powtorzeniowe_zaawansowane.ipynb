{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartynaaP/MartynaaP.github.io/blob/main/zajecia_powtorzeniowe_zaawansowane.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb7b2b0",
      "metadata": {
        "id": "eeb7b2b0"
      },
      "source": [
        "# Zajęcia powtórkowe: Big Data ze Sparkiem (wersja podstawowa)\n",
        "Ten notebook zawiera rozszerzone zadania, wprowadzenie do dodatkowych funkcji Sparka oraz wyjaśnienia kluczowych metod, które pomogą w utrwaleniu materiału.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K49u83lZ4BED",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K49u83lZ4BED",
        "outputId": "f8455be4-803b-444c-8798-3665ffeb3a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Ustaw wersję jako parametr\n",
        "SPARK_VERSION=\"3.5.5\"\n",
        "\n",
        "# Instalacja OpenJDK 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Pobranie Apache Spark z określoną wersją\n",
        "!wget -q http://www.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# Rozpakowanie archiwum Spark\n",
        "!tar xf spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
        "\n",
        "# Instalacja findspark i pyspark\n",
        "!pip install -q findspark==1.3.0\n",
        "!pip install -q pyspark==$SPARK_VERSION\n",
        "\n",
        "# Ustalamy zmienne środowiskowe.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{SPARK_VERSION}-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lk49ARkb4EXv",
      "metadata": {
        "id": "lk49ARkb4EXv"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "\n",
        "findspark.init(f\"spark-{SPARK_VERSION}-bin-hadoop3\")\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "spark = SparkSession.builder.appName('abc').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a8b4742",
      "metadata": {
        "id": "6a8b4742"
      },
      "source": [
        "## Zadanie 1: Agregacja z użyciem Spark DataFrame API i Window Functions\n",
        "\n",
        "**Opis:** Wykorzystaj DataFrame API, aby załadować plik `pracownicy.csv`, a następnie obliczyć dla każdego działu (`department`):\n",
        "1. Liczbę pracowników.\n",
        "2. Średnią stawkę godzinową (`hourly_rate`).\n",
        "3. Dla każdego pracownika dodaj kolumnę z odchyleniem stawki godzinowej od średniej w dziale.\n",
        "\n",
        "**Dodatkowo:**\n",
        "- Zcache'uj (cache) DataFrame przed obliczeniami, żeby pokazać jak działa pamięć podręczna Sparka.\n",
        "- Użyj Window Functions z modułu `pyspark.sql.window`.\n",
        "\n",
        "**Wybrane funkcje do utrwalenia:**\n",
        "- `DataFrame.cache()`: oznacza, że dane zostaną wczytane do pamięci i ponowne operacje będą szybsze.\n",
        "- `Window.partitionBy()`: definiuje, jak dzielimy dane na okna.\n",
        "- `avg`, `col`, `round`: wyrażenia SQL do obliczeń w DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c513b098",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "c513b098",
        "outputId": "cdca30bf-c21d-4640-f984-e7d70e33e8b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CREDIT_LIMIT` cannot be resolved. Did you mean one of the following? [` Imie;Nazwisko ;stawka godzinowa w dolarach;rok urodzenia`].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-589bcde5f33c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"classify_credit_limit_udf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_credit_limit_udf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_classified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREDIT_LIMIT_RANGE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_credit_limit_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CREDIT_LIMIT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf_classified\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUST_ID\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CREDIT_LIMIT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CREDIT_LIMIT_RANGE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3078\u001b[0m         \"\"\"\n\u001b[1;32m   3079\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `CREDIT_LIMIT` cannot be resolved. Did you mean one of the following? [` Imie;Nazwisko ;stawka godzinowa w dolarach;rok urodzenia`]."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "\n",
        "df = spark.read.csv(\"pracownicy.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.createOrReplaceTempView(\"cc\")\n",
        "\n",
        "def classify_credit_limit(credit_limit):\n",
        "    if credit_limit is None:\n",
        "        return None  # albo np. 0, lub inna wartość domyślna\n",
        "    return (credit_limit // 1000) * 1000\n",
        "\n",
        "\n",
        "classify_credit_limit_udf = udf(classify_credit_limit, DoubleType())\n",
        "\n",
        "# Register the UDF with Spark\n",
        "spark.udf.register(\"classify_credit_limit_udf\", classify_credit_limit_udf)\n",
        "\n",
        "df_classified = df.withColumn(\"CREDIT_LIMIT_RANGE\", classify_credit_limit_udf(df[\"CREDIT_LIMIT\"]))\n",
        "\n",
        "df_classified.select(\"CUST_ID\", \"CREDIT_LIMIT\", \"CREDIT_LIMIT_RANGE\").show()\n",
        "\n",
        "df_classified.createOrReplaceTempView(\"cc\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT SUM(INSTALLMENTS_PURCHASES), CREDIT_LIMIT_RANGE\n",
        "FROM cc\n",
        "GROUP BY CREDIT_LIMIT_RANGE\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "buI9r-9wnDZ2"
      },
      "id": "buI9r-9wnDZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad3efd97",
      "metadata": {
        "id": "ad3efd97"
      },
      "source": [
        "## Zadanie 2: Zapytania SQL z UDF i przedziałami\n",
        "\n",
        "**Opis:** Załaduj plik `CC_General.csv` jako DataFrame i zarejestruj tymczasowy widok SQL o nazwie `cc`.\n",
        "1. Zdefiniuj User-Defined Function (UDF), która zaklasyfikuje `credit_limit` do przedziału co 1000.\n",
        "2. Napisz zapytanie SQL, które obliczy sumę `INSTALLMENTS_PURCHASES` dla każdego przedziału.\n",
        "3. Posortuj wynik rosnąco wg przedziału.\n",
        "\n",
        "**Dodatkowo:**\n",
        "- Użyj UDF aby pokazać możliwość rozszerzenia SQL o niestandardowe funkcje.\n",
        "\n",
        "**Wybrane funkcje i moduły:**\n",
        "- `pyspark.sql.functions.udf`: do tworzenia funkcji użytkownika.\n",
        "- `spark.udf.register()`: rejestracja UDF w kontekście SQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f637297b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "f637297b",
        "outputId": "cfd30dbc-9430-4038-8b7d-9529f20b2e0c"
      },
      "outputs": [
        {
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/CC_General.csv.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f4c19787abc7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Zadanie2_Advanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CC_General.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/CC_General.csv."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "\n",
        "df = spark.read.csv(\"CC.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.createOrReplaceTempView(\"cc\")\n",
        "\n",
        "def classify_credit_limit(credit_limit):\n",
        "    if credit_limit is None:\n",
        "        return None  # albo np. 0, lub inna wartość domyślna\n",
        "    return (credit_limit // 1000) * 1000\n",
        "\n",
        "\n",
        "classify_credit_limit_udf = udf(classify_credit_limit, DoubleType())\n",
        "\n",
        "df_classified = df.withColumn(\"CREDIT_LIMIT_RANGE\", classify_credit_limit_udf(df[\"CREDIT_LIMIT\"]))\n",
        "\n",
        "df_classified.select(\"CUST_ID\", \"CREDIT_LIMIT\", \"CREDIT_LIMIT_RANGE\").show()\n",
        "\n",
        "df_classified.createOrReplaceTempView(\"cc\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "SELECT SUM(INSTALLMENTS_PURCHASES), CREDIT_LIMIT_RANGE\n",
        "FROM cc\n",
        "GROUP BY CREDIT_LIMIT_RANGE\n",
        "\"\"\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "183c2914",
      "metadata": {
        "id": "183c2914"
      },
      "source": [
        "## Zadanie 3: Transformacje RDD z Accumulators i Broadcast\n",
        "\n",
        "**Opis:** Wczytaj plik `pracownicy.csv` jako DataFrame i skonwertuj go na RDD.\n",
        "1. Użyj Broadcast Variable, aby przenieść na wszystkie węzły listę działów, które analizujemy.\n",
        "2. Policzyć liczbę pracowników urodzonych po roku 1990 w wybranych działach.\n",
        "3. Użyj Accumulatora, aby zliczyć łączną liczbę przefiltrowanych rekordów.\n",
        "\n",
        "**Wybrane funkcje i moduły:**\n",
        "- `sc.broadcast()`: tworzenie zmiennej broadcast.\n",
        "- `sc.accumulator()`: tworzenie akumulatora do sumowania po stronie sterującej.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ae9932",
      "metadata": {
        "id": "20ae9932"
      },
      "outputs": [],
      "source": [
        "# TODO: Zadanie 3 - Broadcast i Accumulator\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Zadanie3_Advanced').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Lista działów do analizy\n",
        "departments = [] # do uzupełnienia\n",
        "dep_broadcast = sc.broadcast(departments)\n",
        "\n",
        "# Accumulator do zliczania rekordów\n",
        "count_acc = sc.accumulator(0)\n",
        "\n",
        "# Wczytaj dane\n",
        "df = spark.read.option('header', True).csv('pracownicy.csv')\n",
        "rdd = df.rdd\n",
        "\n",
        "def process(row):\n",
        "    # Zwiększ akumulator\n",
        "    count_acc.add(1)\n",
        "    birth_year = int(row['birth_year'])\n",
        "    if birth_year > 1990 and row['department'] in dep_broadcast.value:\n",
        "        return (row['gender'], 1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Filtruj i licz\n",
        "pairs = rdd.map(process).filter(lambda x: x is not None)\n",
        "result = pairs.reduceByKey(lambda a, b: a + b)\n",
        "print('Wynik:', result.collect())\n",
        "print('Przetworzono rekordów:', count_acc.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HBunUKqkBY5j",
      "metadata": {
        "id": "HBunUKqkBY5j"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}